{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Testing openAI API"
      ],
      "metadata": {
        "id": "kzTRk3h7qL6Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sNiMMvxqGyT"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "# creating the model to be used\n",
        "client = OpenAI(api_key=\"{{api_key}}\")\n",
        "# defining the model name and messages to display to check if the key works\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hi, my name is Alex.\"}\n",
        "  ])\n",
        "# printing the first message\n",
        "print(completion.choices[0].message)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM model"
      ],
      "metadata": {
        "id": "m9kG2YMaqRTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "# Initiating the OpenAI LLM with API key\n",
        "llm = OpenAI(api_key=\"{{api_key}}\")\n",
        "# Query the model\n",
        "response = llm.invoke(\"What is the tallest building in the world?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "rnPib0_MqLbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat model"
      ],
      "metadata": {
        "id": "HtqvMJ6TqS-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.messages import HumanMessage, SystemMessage\n",
        "# Initiating the chat model with API key\n",
        "chat = ChatOpenAI(api_key=\"{{api_key}}\")\n",
        "# Defines a context and query using SystemMessage and HumanMessage\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a math tutor who provides answers with a bit of sarcasm.\"),\n",
        "    HumanMessage(content=\"What is the square of 2?\"),\n",
        "]\n",
        "response = chat.invoke(messages)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "Gy5IsAJgqUwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic prompt template"
      ],
      "metadata": {
        "id": "hUwjPRZcqgEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing LangChain modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "# Initiating the chat model with API key\n",
        "llm = OpenAI(openai_api_key = \"{{api_key}}\")\n",
        "email_template = PromptTemplate.from_template(\n",
        "    \"Create an invitation email to the recipinet that is {recipient_name} \\\n",
        " for an event that is {event_type} in a language that is {language} \\\n",
        " Mention the event location that is {event_location} \\\n",
        " and event date that is {event_date}. \\\n",
        " Also write few sentences about the event description that is {event_description} \\\n",
        " in style that is {style} \"\n",
        ")\n",
        "message = email_template.format(\n",
        "    style = \"enthusiastic tone\",\n",
        "    language = \"American english\",\n",
        "    recipient_name=\"John\",\n",
        "    event_type=\"product launch\",\n",
        "    event_date=\"January 15, 2024\",\n",
        "    event_location=\"Grand Ballroom, City Center Hotel\",\n",
        "    event_description=\"an exciting unveiling of our latest innovations\"\n",
        "    )\n",
        "response = llm.invoke(message)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "xpNGgeVtqhjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-shot prompt"
      ],
      "metadata": {
        "id": "izzc2GohqmN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing LangChain modules\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "# Initiating the chat model with API key\n",
        "chat = ChatOpenAI(temperature=0.0, openai_api_key = \"{{api_key}}\")\n",
        "examples = [\n",
        "  {\n",
        "    \"review\": \"I absolutely love this product! It exceeded my expectations.\",\n",
        "    \"sentiment\": \"Positive\"\n",
        "  },\n",
        "  {\n",
        "    \"review\": \"I'm really disappointed with the quality of this item. It didn't meet my needs.\",\n",
        "    \"sentiment\": \"Negative\"\n",
        "  },\n",
        "  {\n",
        "    \"review\": \"The product is okay, but there's room for improvement.\",\n",
        "    \"sentiment\": \"Neutral\"\n",
        "  }\n",
        "]\n",
        "example_prompt = PromptTemplate(\n",
        "                        input_variables=[\"review\", \"sentiment\"],\n",
        "                        template=\"Review: {review}\\n{sentiment}\")\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Review: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "message = prompt.format(input=\"The machine worked okay without much trouble.\")\n",
        "\n",
        "response = chat.invoke(message)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "oTzKGv9jqr2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datetime parser"
      ],
      "metadata": {
        "id": "7G0ZKEN2qyso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import DatetimeOutputParser\n",
        "parser_dateTime = DatetimeOutputParser()\n",
        "# creating our prompt template\n",
        "template = \"\"\"Provide the response in format {format_instructions}\n",
        "            to the user's question {question}\"\"\"\n",
        "\n",
        "# passing the format instructions of the parser to the template\n",
        "prompt_dateTime = PromptTemplate.from_template(\n",
        "    template,\n",
        "    partial_variables={\"format_instructions\": parser_dateTime.get_format_instructions()},\n",
        ")\n",
        "print(llm.predict(text = prompt_dateTime.format(question=\"ENTER_YOUR_QUERY_HERE\")))"
      ],
      "metadata": {
        "id": "bESYxVq5q4W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List parser"
      ],
      "metadata": {
        "id": "KV0xN_SjrX0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "parser_List = CommaSeparatedListOutputParser()\n",
        "partial_variables={\"format_instructions\": parser_List.get_format_instructions()}\n",
        "print(llm.predict(text = prompt_List.format(question=\"ENTER_YOUR_QUERY_HERE\")))"
      ],
      "metadata": {
        "id": "gX9EhT07rWAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datetime and list parsers"
      ],
      "metadata": {
        "id": "bVWf-SkHrpJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing LangChain modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import DatetimeOutputParser, CommaSeparatedListOutputParser\n",
        "\n",
        "\n",
        "# Insert your key here\n",
        "llm = OpenAI(openai_api_key = \"{{api_key}}\")\n",
        "\n",
        "parser_dateTime = DatetimeOutputParser()\n",
        "parser_List = CommaSeparatedListOutputParser()\n",
        "\n",
        "# creating our prompt template\n",
        "template = \"\"\"Provide the response in format {format_instructions}\n",
        "            to the user's question {question}\"\"\"\n",
        "\n",
        "prompt_dateTime = PromptTemplate.from_template(\n",
        "    template,\n",
        "    partial_variables={\"format_instructions\": parser_dateTime.get_format_instructions()},\n",
        ")\n",
        "\n",
        "prompt_List = PromptTemplate.from_template(\n",
        "    template,\n",
        "    partial_variables={\"format_instructions\": parser_List.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# formatting the output\n",
        "print(llm.predict(text = prompt_dateTime.format(question=\"When was the first iPhone launched?\")))\n",
        "print(llm.predict(text = prompt_List.format(question=\"What are the four famous chocolate brands?\")))"
      ],
      "metadata": {
        "id": "Dmz0_XhRrp0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pydantic (JSON) parser"
      ],
      "metadata": {
        "id": "qOXZYCWms2Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "\n",
        "# insert your key here\n",
        "model = OpenAI(openai_api_key = \"{{api_key}}\")\n",
        "\n",
        "# defining the author class for the model\n",
        "class Author(BaseModel):\n",
        "    number: int = Field(description=\"number of books written by the author\")\n",
        "    books: List[str] = Field(description=\"list of books they wrote\")\n",
        "\n",
        "user_query = \"Generate the books written by Dan Brown.\"\n",
        "\n",
        "# defining the output parser\n",
        "output_parser = PydanticOutputParser(pydantic_object=Author)\n",
        "\n",
        "# defining the prompt template with parser instructions\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# defining the prompt\n",
        "my_prompt = prompt.format_prompt(query=user_query)\n",
        "\n",
        "# coverting the output to a string while generating the response\n",
        "output = model(my_prompt.to_string())\n",
        "\n",
        "# printing the result\n",
        "print(output_parser.parse(output))"
      ],
      "metadata": {
        "id": "V9VTOhT2s2x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Chain"
      ],
      "metadata": {
        "id": "sma25CFht4vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# defining the LLM model\n",
        "llm = OpenAI(temperature=0.0, openai_api_key=\"{{api_key}}\")\n",
        "\n",
        "# creating the prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"book\"],\n",
        "    template=\"Name the author of the book {book}?\",\n",
        ")\n",
        "\n",
        "# creating the chain\n",
        "chain = LLMChain(llm=llm,\n",
        "                prompt=prompt_template,\n",
        "                verbose=True)\n",
        "\n",
        "# calling the chain\n",
        "print(chain.run(\"The Da Vinci Code\"))"
      ],
      "metadata": {
        "id": "sZG1Hjy2tCQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple sequential chain"
      ],
      "metadata": {
        "id": "S69cWv7vt6rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# defining the LLM model for the first chain\n",
        "llm = OpenAI(temperature=0.0, openai_api_key=\"{{api_key}}\")\n",
        "\n",
        "# creating the prompt template and the first chain\n",
        "prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"book\"],\n",
        "    template=\"Name the author who wrote the book {book}?\"\n",
        ")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
        "\n",
        "# creating the prompt template and the second chain\n",
        "prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"author_name\"],\n",
        "    template=\"Write a 50-word biography for the following author:{author_name}\"\n",
        ")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
        "\n",
        "# combining the chains into a simple sequential chain\n",
        "simple_sequential_chain = SimpleSequentialChain(chains=[chain_1, chain_2],verbose=True)\n",
        "\n",
        "# running the simple sequential chain\n",
        "simple_sequential_chain.run(\"The Da Vinci Code\")"
      ],
      "metadata": {
        "id": "Ozxh3wWAt6cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple input sequential chain"
      ],
      "metadata": {
        "id": "4zchxmISuZb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# defining the LLM model\n",
        "llm = OpenAI(temperature=0.0, openai_api_key=\"{{api_key}}\")\n",
        "\n",
        "# defining the original input\n",
        "biography = \"He is an American author of thriller fiction, best known for his Robert Langdon series. \\\n",
        "          He has sold over 200 million copies of his books, which have been translated into 56 \\\n",
        "          languages. His other works include Angels & Demons, The Lost Symbol, Inferno, and Origin. \\\n",
        "          He is a New York Times best-selling author and has been awarded numerous awards for his \\\n",
        "          writing.\"\n",
        "\n",
        "# creating the prompt template for the first chain\n",
        "prompt_1 = ChatPromptTemplate.from_template(\n",
        "    \"Summarize this biography in one sentence:\"\n",
        "    \"\\n\\n{biography}\"\n",
        ")\n",
        "\n",
        "# we input the original biography, and the output here is the one-line biography\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"one_line_biography\")\n",
        "\n",
        "# creating the prompt template for the second chain\n",
        "prompt_2 = ChatPromptTemplate.from_template(\n",
        "    \"Can you tell the author's name in this biography:\"\n",
        "    \"\\n\\n{one_line_biography}\"\n",
        ")\n",
        "\n",
        "# we input the one-line biography, and the output here is the author's name\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2, output_key=\"author_name\")\n",
        "\n",
        "# creating the prompt template for the third chain\n",
        "prompt_3 = ChatPromptTemplate.from_template(\n",
        "    \"Tell the name of the highest selling book of this author: \"\n",
        "    \"\\n\\n{author_name}\"\n",
        ")\n",
        "\n",
        "# we input the author's name, and the output here is the highest-selling book\n",
        "chain_3 = LLMChain(llm=llm, prompt=prompt_3, output_key=\"book\")\n",
        "\n",
        "# creating the prompt template for the fourth chain\n",
        "prompt_4 = ChatPromptTemplate.from_template(\n",
        "   \"Write a follow-up response to the following \"\n",
        "    \"summary of the highest-selling book of the author:\"\n",
        "    \"\\n\\nAuthor: {author_name}\\n\\nBook: {book}\"\n",
        ")\n",
        "\n",
        "# we input the author's name and the highest-selling book, and the output is the book's summary\n",
        "chain_4 = LLMChain(llm=llm, prompt=prompt_4, output_key=\"summary\")\n",
        "\n",
        "# combining the chains\n",
        "final_chain = SequentialChain(\n",
        "    chains=[chain_1, chain_2, chain_3, chain_4],\n",
        "    input_variables=[\"biography\"],\n",
        "    output_variables=[\"one_line_biography\", \"author_name\",\"summary\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# calling the final chain\n",
        "print(final_chain(biography))"
      ],
      "metadata": {
        "id": "U1_51jyYudzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatBot using router chain"
      ],
      "metadata": {
        "id": "VzTqtsktvSsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "\n",
        "# Defining prompt templates for the destination chains\n",
        "shipping_template = \"\"\"You are the shipping manager of a company. \\\n",
        "As a shipping customer service agent, respond to a customer inquiry about the current status \\\n",
        "and estimated delivery time of their package. Include details about the \\\n",
        "shipping route and any potential delays, providing a comprehensive and reassuring response. \\\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "billing_template = \"\"\"You are the billing manager of a company. \\\n",
        "Address a customer's inquiry regarding an unexpected charge on their account.\\\n",
        "Explain the nature of the charge and any relevant billing policies, and promptly resolve \\\n",
        "the concern to ensure customer satisfaction. Additionally, offer guidance on how the \\\n",
        "customer can monitor and manage their billing preferences moving forward.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "technical_template = \"\"\"You are very good at understanding the technology of your company's product. \\\n",
        "Assist a customer experiencing issues with a software application. \\\n",
        "Walk them through troubleshooting steps, provide clear instructions \\\n",
        "on potential solutions, and ensure the customer feels supported\\\n",
        "throughout the process. Additionally, offer guidance on preventive \\\n",
        "measures to minimize future technical issues and optimize their \\\n",
        "experience with the software.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "# storing the prompt templates in prompt_infos\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"Shipping\",\n",
        "        \"description\": \"Good for answering questions about shipping issues of a product\",\n",
        "        \"prompt_template\": shipping_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Billing\",\n",
        "        \"description\": \"Good for answering questions regarding billing issues of a product\",\n",
        "        \"prompt_template\": billing_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Technical\",\n",
        "        \"description\": \"Good for answering questions regarding technical issues of a product\",\n",
        "        \"prompt_template\": technical_template\n",
        "    }\n",
        "]\n",
        "\n",
        "# creating the destination chains\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "# creating the default chain\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
        "\n",
        "# defining the router template\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "# creating the router chain\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "# combining the chains\n",
        "final_chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )\n",
        "\n",
        "# running the combined chain\n",
        "print(final_chain.run(\"The package was supposed to arrive last week and I haven't received it yet. I want to cancel my order NOW!\"))"
      ],
      "metadata": {
        "id": "xW3uDWJ3vUeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation buffer"
      ],
      "metadata": {
        "id": "ub-SzeMXwRS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing LangChain modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "\n",
        "# Insert your key here\n",
        "llm = OpenAI(temperature=0.0,\n",
        "            openai_api_key = \"{{api_key}}\")\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "memory.save_context({\"input\": \"Alex is a 9-year old boy.\"},\n",
        "                    {\"output\": \"Hello Alex! How can I assist you today?\"})\n",
        "memory.save_context({\"input\": \"Alex likes to play football\"},\n",
        "                    {\"output\": \"That's great to hear! \"})\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(conversation.predict(input=\"How old is Alex?\"))"
      ],
      "metadata": {
        "id": "0VShp1NYwTT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation window"
      ],
      "metadata": {
        "id": "FFsvGmAywdfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing LangChain modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "\n",
        "# Insert your key here\n",
        "llm = OpenAI(temperature=0.0,\n",
        "            openai_api_key = \"{{api_key}}\")\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "memory.save_context({\"input\": \"Alex is a 9-year old boy.\"},\n",
        "                    {\"output\": \"Hello Alex! How can I assist you today?\"})\n",
        "memory.save_context({\"input\": \"Alex likes to play football\"},\n",
        "                    {\"output\": \"That's great to hear! \"})\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(conversation.predict(input=\"How old is Alex?\"))"
      ],
      "metadata": {
        "id": "5pgJJuA3wWCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation summary"
      ],
      "metadata": {
        "id": "SFAG2hvbwwc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing LangChain modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "\n",
        "# Insert your key here\n",
        "llm = OpenAI(temperature=0.0,\n",
        "            openai_api_key = \"{{api_key}}\")\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "memory.save_context({\"input\": \"Alex is a 9-year old boy.\"},\n",
        "                    {\"output\": \"Hello Alex! How can I assist you today?\"})\n",
        "memory.save_context({\"input\": \"Alex likes to play football\"},\n",
        "                    {\"output\": \"That's great to hear! \"})\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(conversation.predict(input=\"How old is Alex?\"))"
      ],
      "metadata": {
        "id": "-p5Y1lOnwya_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A chatbot that identifies the language of the user queries and answers them accordingly using the router chain."
      ],
      "metadata": {
        "id": "prypIx6bxVqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "\n",
        "# defining the LLM model\n",
        "llm = OpenAI(temperature=0.0, openai_api_key=\"{{api_key}}\")\n",
        "\n",
        "# Defining prompt templates for the destination chains\n",
        "french_template = \"\"\"You are proficient in the French language and are very knowledgeable. \\\n",
        " Translate the customer query to English and then respond to the user in French. \\\n",
        " Be polite and have a friendly tone. \\\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "spanish_template = \"\"\"You are proficient in the Spanish language and are very knowledgeable. \\\n",
        " Translate the user query to English and then respond to the user in Spanish. \\\n",
        " Be polite and have a friendly tone. \\\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "dutch_template = \"\"\"You are proficient in the Dutch language and are very knowledgeable. \\\n",
        " Translate the user query to English and then respond to the user in Dutch. \\\n",
        " Be polite and have a friendly tone. \\\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "italian_template = \"\"\"You are proficient in the Italian language and are very knowledgeable. \\\n",
        " Translate the user query to English and then respond to the user in Italian. \\\n",
        " Be polite and have a friendly tone. \\\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "# storing the prompt templates in prompt_infos\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"French\",\n",
        "        \"description\": \"Good for answering questions in French\",\n",
        "        \"prompt_template\": french_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Spanish\",\n",
        "        \"description\": \"Good for answering questions in Spanish\",\n",
        "        \"prompt_template\": spanish_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Dutch\",\n",
        "        \"description\": \"Good for answering questions in Dutch\",\n",
        "        \"prompt_template\": dutch_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Italian\",\n",
        "        \"description\": \"Good for answering questions in Italian\",\n",
        "        \"prompt_template\": italian_template\n",
        "    }\n",
        "]\n",
        "\n",
        "# creating the destination chains\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "# creating the default chain\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
        "\n",
        "# defining the router template\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "\n",
        "# defining the router prompt template\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "# creating the router chain\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "# combining the chains\n",
        "final_chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )\n",
        "\n",
        "# running the combined chain\n",
        "print(final_chain.run(\"Cos'è un atomo?\"))"
      ],
      "metadata": {
        "id": "XL3YrcUKxYIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting information from an external API - React agent"
      ],
      "metadata": {
        "id": "SoBpk1hyxqnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# importing LangChain modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"{{SERPAPI_API_KEY}}\"\n",
        "\n",
        "# Insert your key here\n",
        "llm = OpenAI(temperature=0.0,\n",
        "            openai_api_key = \"{{api_key}}\")\n",
        "\n",
        "# loading tools\n",
        "tools = load_tools([\"serpapi\",\n",
        "                    \"llm-math\"],\n",
        "                    llm=llm)\n",
        "\n",
        "agent = initialize_agent(tools,\n",
        "                        llm,\n",
        "                        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                        verbose=True)\n",
        "\n",
        "# user's query\n",
        "print(agent.run(\"What is the current population of the world, and calculate the percentage change compared to the population five years ago\"))"
      ],
      "metadata": {
        "id": "RgIvkwjBxqcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empowering ChatGPT with current knowledge - Conversational react"
      ],
      "metadata": {
        "id": "kDGE09lgynHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# importing LangChain modules\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"{{SERPAPI_API_KEY}}\"\n",
        "# Insert your key here\n",
        "llm = OpenAI(temperature=0.0,\n",
        "            openai_api_key = \"{{api_key}}\")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# loading tools\n",
        "tools = load_tools([\"serpapi\"],\n",
        "                    llm=llm)\n",
        "\n",
        "agent = initialize_agent(tools,\n",
        "                        llm,\n",
        "                        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "                        verbose=True,\n",
        "                        memory=memory)\n",
        "\n",
        "agent.run(\"Hi, my name is Alex, and I live in the New York City.\")\n",
        "agent.run(\"My favorite game is basketball.\")\n",
        "print(agent.run(\"Give me the list of stadiums to watch a basketball game in my city today. Also give the teams that are playing.\"))"
      ],
      "metadata": {
        "id": "8hyqLVPcymgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document loaders"
      ],
      "metadata": {
        "id": "uZ2VOkM6ymOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a text file\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"inputFile.txt\")\n",
        "loader.load()\n",
        "\n",
        "# Loading a csv file\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "loader = CSVLoader(file_path='inputRecord.csv')\n",
        "data = loader.load()\n",
        "\n",
        "# Loading a pdf file\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"inputDocument.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "-iGwmzUyz7mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document transformers"
      ],
      "metadata": {
        "id": "7B56c6tH0EsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # The chunk_size and chunk_overlap can be modified according to the requirements\n",
        "    length_function = len,\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap  = 10,\n",
        "    add_start_index = True,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([input_document])"
      ],
      "metadata": {
        "id": "-SdXLBnV0G4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text embedding"
      ],
      "metadata": {
        "id": "2440GfW11Kjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding doc\n",
        "embeddings = embeddings_model.embed_documents(\n",
        " [\n",
        "     \"This is the first text\",\n",
        "     \"This is the second text\",\n",
        "     \"This is the third text\"\n",
        " ]\n",
        ")\n",
        "\n",
        "# embedding a query\n",
        "embedded_query = embeddings_model.embed_query(\"WRITE_YOUR_QUERY_HERE\")"
      ],
      "metadata": {
        "id": "vlGBtPuu1MI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing a simple similarity index for searching data"
      ],
      "metadata": {
        "id": "62m_v-c31U5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# Chroma module is an open-source vector store for building AI applications.\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Load the document\n",
        "input_document = TextLoader('my_document.txt').load()\n",
        "\n",
        "# transform the document\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # The chunk_size and chunk_overlap can be modified according to the requirements\n",
        "    length_function = len,\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap  = 10,\n",
        "    add_start_index = True,\n",
        ")\n",
        "\n",
        "documents = text_splitter.create_documents([input_document])\n",
        "\n",
        "# embed the chunks\n",
        "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
        "\n",
        "# user query\n",
        "query = \"WRITE_YOUR_QUERY_HERE\"\n",
        "\n",
        "# computing the search using the similarity_search() method\n",
        "docs = db.similarity_search(query)"
      ],
      "metadata": {
        "id": "rNTNla1S1xK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using vector stores to compute the similarity"
      ],
      "metadata": {
        "id": "ihhSymIl2QTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Load the document\n",
        "input_document = TextLoader('my_document.txt').load()\n",
        "\n",
        "# transform the document\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # The chunk_size and chunk_overlap can be modified according to the requirements\n",
        "    length_function = len,\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap  = 10,\n",
        "    add_start_index = True,\n",
        ")\n",
        "\n",
        "documents = text_splitter.create_documents([input_document])\n",
        "\n",
        "# embed the chunks\n",
        "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
        "\n",
        "# user query\n",
        "query = \"WRITE_YOUR_QUERY_HERE\"\n",
        "\n",
        "# embedding the query\n",
        "embedding_vector = OpenAIEmbeddings().embed_query(query)\n",
        "\n",
        "# computing the search using the search_by_vector() method\n",
        "docs = db.similarity_search_by_vector(embedding_vector)"
      ],
      "metadata": {
        "id": "gXXOxgKX2Q9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrievers"
      ],
      "metadata": {
        "id": "t20FA-oq25ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_texts(texts, embeddings)\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "# invoking the retriever\n",
        "retrieved_docs = retriever.invoke(\n",
        "    # write your query here\n",
        ")"
      ],
      "metadata": {
        "id": "zlJ9JW8x263E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using RAG to implement question answering\n",
        "We’ll build a chatbot that answers the user’s queries using the PDF and a memory of the conversation history."
      ],
      "metadata": {
        "id": "oRHkguDd3ae2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# loading the document\n",
        "loader = PyPDFLoader(\"./3DPrinter_Manual.pdf\")\n",
        "mypdf = loader.load()"
      ],
      "metadata": {
        "id": "ZYKJA4iA3bRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Defining the splitter\n",
        "document_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 300,\n",
        "    chunk_overlap = 70\n",
        ")\n",
        "\n",
        "# splitting the document\n",
        "docs = document_splitter.split_documents(mypdf)"
      ],
      "metadata": {
        "id": "dJ6D_5hE3fIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# embedding the chunks to vector stores\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=\"{{api_key}}\")\n",
        "persist_directory = 'db'\n",
        "\n",
        "my_database = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ],
      "metadata": {
        "id": "dTcqSatl3if1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# defining the conversational memory\n",
        "retaining_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=5,\n",
        "    return_messages=True\n",
        ")"
      ],
      "metadata": {
        "id": "YQjnG0CE3kfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# defining the retriever\n",
        "question_answering = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=my_database.as_retriever(),\n",
        "    memory=retaining_memory\n",
        ")"
      ],
      "metadata": {
        "id": "fXB2Z6v73mcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the loop for a conversation with the AI\n",
        "while True:\n",
        "    question = input(\"Enter your query: \")\n",
        "    if question == 'exit':\n",
        "\t    break\n",
        "    # getting the response\n",
        "    result = question_answering({\"question\": \"Answer only in the context of the document provided.\" + question})\n",
        "    print(result['answer'])"
      ],
      "metadata": {
        "id": "emfJF6SF3oaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents for Question Answering with RAG"
      ],
      "metadata": {
        "id": "dafyGP0T36tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the modules\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# defining the model\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=\"{{api_key}}\",\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "# loading the document\n",
        "loader = PyPDFLoader(\"./3DPrinter_Manual.pdf\")\n",
        "mypdf = loader.load()\n",
        "\n",
        "# Defining the splitter\n",
        "document_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 300,\n",
        "    chunk_overlap = 70\n",
        ")\n",
        "\n",
        "# splitting the document\n",
        "docs = document_splitter.split_documents(mypdf)\n",
        "\n",
        "# embedding the chunks to vector stores\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=\"{{api_key}}\")\n",
        "persist_directory = 'db'\n",
        "\n",
        "my_database = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "# defining the conversational memory\n",
        "retaining_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=5,\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# defining the retriever\n",
        "question_answering = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=my_database.as_retriever(),\n",
        "    memory=retaining_memory\n",
        ")\n",
        "\n",
        "# defining the tool for the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name='Knowledge Base',\n",
        "        func=question_answering.run,\n",
        "        description=(\n",
        "            'use this tool when answering questions related to the 3D printer'\n",
        "        )\n",
        "    )\n",
        "]\n",
        "\n",
        "# initializing the agent\n",
        "agent = initialize_agent(\n",
        "    agent='chat-conversational-react-description',\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method='generate',\n",
        "    memory=retaining_memory\n",
        ")\n",
        "\n",
        "# calling the agent\n",
        "while True:\n",
        "    question = input(\"Enter your query: \")\n",
        "    if question == 'exit':\n",
        "\t    break\n",
        "    print(agent(question))\n"
      ],
      "metadata": {
        "id": "ab3xRhY137M_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}